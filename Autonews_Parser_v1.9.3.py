import json
import time
import threading
import traceback
import telebot
import random
from playwright.sync_api import sync_playwright
from sentence_transformers import SentenceTransformer, util
import numpy as np
import logging
import sys
import torch
import requests
import hashlib
from datetime import datetime, timedelta, timezone
import telebot.apihelper
from sentence_transformers import CrossEncoder
import faiss
import os

log_file = "log_parser.txt"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_file, mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

def log(msg):
    logging.info(msg)

model = SentenceTransformer('all-mpnet-base-v2')
sent_error_hashes = set()
session_embeddings = []
SESSION_EMBEDDING_LIMIT = 50
cross_model = CrossEncoder("cross-encoder/stsb-roberta-large")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
TELEGRAM_TOKEN = ''
TELEGRAM_CHAT_ID = ''
PUBLISHED_FILE = "published_articles.json"
ADMIN_CHAT_ID = ''
OPENROUTER_API_KEY = ''

bot = telebot.TeleBot(TELEGRAM_TOKEN)
post_queue = []
all_parsed_articles = []
last_sent_time = 0
faiss_index = None
faiss_texts = []

def safe_decode(text):
    if not text:
        return text
    try:
        return text.encode('latin1').decode('utf-8')
    except (UnicodeEncodeError, AttributeError):
        return text

def is_valid_image_url(url):
    try:
        r = requests.head(url, timeout=5, allow_redirects=True)
        content_type = r.headers.get("Content-Type", "")
        return content_type.startswith("image/")
    except:
        return False

def notify_admin(text, image_url=None, stop_on_error=False):
    error_hash = hashlib.md5(text.encode("utf-8")).hexdigest()

    if error_hash in sent_error_hashes:
        log(f"üö´ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–æ—Å—å: {text.splitlines()[0]}")
        return

    sent_error_hashes.add(error_hash)

    try:
        if image_url and is_valid_image_url(image_url):
            bot.send_photo(chat_id=ADMIN_CHAT_ID, photo=image_url, caption=text, parse_mode="HTML")
        else:
            bot.send_message(chat_id=ADMIN_CHAT_ID, text=text, parse_mode="HTML")
        log("üîî –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∞–¥–º–∏–Ω—É")

        if stop_on_error:
            log("‚õî –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω—É.")
            sys.exit(1)

    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω—É: {e}")
        if stop_on_error:
            log("‚õî –°–∫—Ä–∏–ø—Ç –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è.")
            sys.exit(1)

def load_published():
    if not os.path.exists(PUBLISHED_FILE):
        return []
    with open(PUBLISHED_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_published(data):
    with open(PUBLISHED_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def build_faiss_index(published_data):
    dimension = 768
    index = faiss.IndexFlatIP(dimension)  # —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞
    texts = []
    vectors = []

    for item in published_data:
        if "embedding" in item:
            emb = np.array(item["embedding"], dtype="float32")
            norm = np.linalg.norm(emb)
            if norm != 0:
                emb = emb / norm
            vectors.append(emb)
            texts.append((item.get("link", ""), item.get("text", "")))

    if vectors:
        index.add(np.vstack(vectors))
        log(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(vectors)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    else:
        log(f"‚ö†Ô∏è FAISS: –Ω–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ (—Å–æ–∑–¥–∞–Ω –ø—É—Å—Ç–æ–π)")

    return index, texts

def is_duplicate(title, lead, published_data, threshold_faiss=0.9, threshold_cross=0.9, llm_min=0.8):
    global faiss_index, faiss_texts
    if not title or not lead:
        log("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–∞: –ø—É—Å—Ç–æ–π title –∏–ª–∏ lead")
        return False

    text = f"{title.strip()} {lead.strip()}".lower()
    embedding = model.encode(text, convert_to_numpy=True, normalize_embeddings=True)

    # -------------------- FAISS --------------------
    if faiss_index is None or faiss_index.ntotal == 0:
        log(f"‚ö†Ô∏è FAISS –∏–Ω–¥–µ–∫—Å –ø—É—Å—Ç, –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
        return False

    D, I = faiss_index.search(np.expand_dims(embedding, axis=0), k=faiss_index.ntotal)
    cross_candidates = []

    for score, idx in zip(D[0], I[0]):
        score = float(score)
        if idx >= len(faiss_texts):
            continue

        top_link, top_text = faiss_texts[idx]
        log(f"üîç FAISS —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å {top_link} ‚Äî score: {score:.3f}")

        if score >= threshold_faiss:
            log(f"‚ö†Ô∏è –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ FAISS –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç")
            return True

        cross_candidates.append((top_link, top_text, score))

    # -------------------- CrossEncoder --------------------
    if not cross_candidates:
        return False

    # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ø-5 –ø–æ FAISS
    cross_candidates = sorted(cross_candidates, key=lambda x: x[2], reverse=True)[:5]
    llm_candidates = []

    for top_link, top_text, faiss_score in cross_candidates:
        cross_score = cross_model.predict([(text, top_text)])[0]
        log(f"üîç CrossEncoder —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å {top_link} ‚Äî score: {cross_score:.3f}")

        if cross_score >= threshold_cross:
            log(f"‚ö†Ô∏è –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ CrossEncoder –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç")
            return True

        if llm_min <= cross_score < threshold_cross:
            llm_candidates.append((top_link, top_text, cross_score))

    # -------------------- LLM --------------------
    # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ø-2 –¥–ª—è LLM
    llm_candidates = sorted(llm_candidates, key=lambda x: x[2], reverse=True)[:2]

    for top_link, top_text, cross_score in llm_candidates:
        llm_result = llm_check(text, top_text)
        log(f"üîç LLM —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å {top_link}, —Ç–µ–∫—Å—Ç: {text[:20]}...\n, —Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã: {top_text[:20]}  ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {llm_result}")

        if llm_result is True:
            log(f"‚ö†Ô∏è –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ LLM –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç")
            return True

    return False

def enqueue_post(title, lead, image_url, link, source_name):
    if not title or not lead:
        return
    post = {
        "title": title.strip(),
        "lead": lead[:500].strip(),
        "image_url": image_url if image_url and image_url.startswith('http') else "https://via.placeholder.com/800x400?text=No+Image",
        "link": link,
        "source": source_name
    }
    post_queue.append(post)
    log(f"üìù –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {post['title']}")

def publish_loop():
    global faiss_index, faiss_texts
    global last_sent_time
    while True:
        random.shuffle(post_queue)
        if post_queue:
            post = post_queue.pop(0)
            try:
                caption = (
                    f"<b>{post['title']}</b>\n\n"
                    f"{post['lead']}\n\n"
                    f"–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"{post['link']}\">{post['source']}</a>"
                )

                if is_valid_image_url(post["image_url"]):
                    bot.send_photo(
                        chat_id=TELEGRAM_CHAT_ID,
                        photo=post["image_url"],
                        caption=caption,
                        parse_mode="HTML"
                    )
                    log(f"üì§ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {post['title']}")
                else:
                    log(f"‚ö†Ô∏è –ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {post['image_url']}. –û—Ç–ø—Ä–∞–≤–∫–∞ –±–µ–∑ —Ñ–æ—Ç–æ.")
                    bot.send_message(
                        chat_id=TELEGRAM_CHAT_ID,
                        text=caption,
                        parse_mode="HTML"
                    )
                    log(f"üì§ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {post['title']}")

                # ‚è±Ô∏è –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ—Ç–ø—Ä–∞–≤–∫–∏
                last_sent_time = time.time()

                # üß† –î–æ–±–∞–≤–ª—è–µ–º –≤ published_articles
                embedding = model.encode(f"{post['title']} {post['lead']}", convert_to_numpy=True).tolist()
                published_data = load_published()
                new_entry = {
                    "title": post["title"],
                    "lead": post["lead"],
                    "link": post["link"],
                    "embedding": embedding,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "text": f"{post['title']} {post['lead']}".lower()
                }
                published_data.append(new_entry)
                save_published(published_data)

                # üî• –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ FAISS
                new_emb = np.array(embedding, dtype="float32")
                norm = np.linalg.norm(new_emb)
                if norm != 0:
                    new_emb = new_emb / norm
                if faiss_index is None:
                    dimension = 768
                    faiss_index = faiss.IndexFlatIP(dimension)
                faiss_index.add(np.expand_dims(new_emb, axis=0))
                faiss_texts.append((post["link"], new_entry["text"]))

                # ‚è≥ –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏
                time.sleep(600)

            except telebot.apihelper.ApiTelegramException as e:
                log(f"‚ùå –û—à–∏–±–∫–∞ Telegram API: {e}")
                notify_admin(f"<b>‚ùå –û—à–∏–±–∫–∞ Telegram API</b>\n\n{e}\n\n<i>{post['title']}</i>", post["image_url"])
                post_queue.insert(0, post)
                time.sleep(60)

            except Exception as e:
                log(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
                notify_admin(f"<b>‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</b>\n\n{e}\n\n<i>{post['title']}</i>", post["image_url"])
                post_queue.insert(0, post)
                time.sleep(60)

        else:
            time.sleep(5)

# === –ü–ê–†–°–ò–ù–ì kolesa.ru ===

def parse_kolesa_ru(page):
    log("üåê kolesa.ru/news")
    try:
        page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Accept-Language': 'ru-RU,ru;q=0.9'
        })
        page.goto("https://www.kolesa.ru/news", timeout=40000, wait_until="domcontentloaded")
        time.sleep(3)
        links = page.evaluate("""() => {
            const links = new Set();
            const xpathResult = document.evaluate(
                "//a[contains(@href, '/news/') and not(contains(@href, '/news/archive/'))]",
                document,
                null,
                XPathResult.ORDERED_NODE_SNAPSHOT_TYPE,
                null
            );
            for (let i = 0; i < Math.min(xpathResult.snapshotLength, 15); i++) {
                const link = xpathResult.snapshotItem(i);
                if (link.textContent.trim().length > 10) {
                    links.add(link.href.startsWith('http') ? link.href : 'https://www.kolesa.ru' + link.href);
                }
            }
            return Array.from(links);
        }""")
        log(f"‚úÖ kolesa.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return links[:5]
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ kolesa.ru: {e}")
        return []

def parse_kolesa_article(page, url):
    try:
        page.goto(url, timeout=30000, wait_until="domcontentloaded")
        time.sleep(3)

        title = page.locator("h1").first.inner_text(timeout=5000)
        lead = ""
        paragraphs = page.locator("p")
        count = paragraphs.count()
        for i in range(count):
            try:
                text = paragraphs.nth(i).inner_text(timeout=1000).strip()
                if len(text) > 30:
                    lead = text
                    break
            except:
                continue

        title = safe_decode(title)
        lead = safe_decode(lead)

        image_url = page.locator("meta[property='og:image']").get_attribute("content")
        if not image_url:
            image_url = page.locator("img").first.get_attribute("src")

        if not title or not lead:
            log(f"‚ö†Ô∏è kolesa.ru: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}")
            return None, None, None
        return title.strip(), lead, image_url
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ kolesa.ru: {e}")
        return None, None, None

# === –ü–ê–†–°–ò–ù–ì autostat.ru ===

def parse_autostat_ru(page):
    log("üåê autostat.ru")
    try:
        page.goto("https://www.autostat.ru/news/", timeout=60000)
        time.sleep(5)
        links = page.evaluate("""() => {
            const found = [];
            document.querySelectorAll('a[href^="/news/"]').forEach(a => {
                const href = a.getAttribute('href');
                if (/^\\/news\\/\\d+\\/$/.test(href)) {
                    found.push('https://www.autostat.ru' + href);
                }
            });
            return found.slice(0, 10);
        }""")
        log(f"‚úÖ autostat.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return links[:5]
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ autostat.ru: {str(e)[:200]}")
        return []

def parse_autostat_article(page, url):
    try:
        page.goto(url, timeout=30000)
        time.sleep(3)

        title = page.locator("h1").first.inner_text(timeout=5000)
        lead = ""
        paragraphs = page.locator("p")
        count = paragraphs.count()
        for i in range(count):
            try:
                text = paragraphs.nth(i).inner_text(timeout=1000).strip()
                if (
                    50 < len(text) < 500
                    and "e-mail" not in text.lower()
                    and "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è" not in text.lower()
                    and "–Ω–∞–∂–º–∏—Ç–µ" not in text.lower()
                    and "–ø–æ–¥–ø–∏—Å" not in text.lower()
                    and "–∏—Å—Ç–æ—á–Ω–∏–∫" not in text.lower()
                ):
                    lead = text
                    break
            except:
                continue

        title = safe_decode(title)
        lead = safe_decode(lead)

        image_url = page.locator("meta[property='og:image']").get_attribute("content")
        if not image_url:
            image_url = page.locator("img").first.get_attribute("src")

        if not title or not lead:
            log(f"‚ö†Ô∏è autostat.ru: –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}")
            return None, None, None
        return title.strip(), lead, image_url
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ autostat.ru: {e}")
        return None, None, None

# === –ü–ê–†–°–ò–ù–ì avtonovostidnya.ru ===
def parse_avtonovostidnya_ru(page):
    log("üåê avtonovostidnya.ru")
    try:
        page.goto("https://avtonovostidnya.ru/", timeout=30000)
        time.sleep(3)
        links = page.evaluate("""() => {
            const items = Array.from(document.querySelectorAll('article'));
            return items.map(item => {
                const link = item.querySelector('h2 a, h3 a');
                return link ? link.href : null;
            }).filter(href => href && href.includes('avtonovostidnya.ru')).slice(0, 5);
        }""")
        log(f"‚úÖ avtonovostidnya.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return links
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ avtonovostidnya.ru: {str(e)[:200]}")
        return []

def parse_avtonovostidnya_article(page, url):
    try:
        page.goto(url, timeout=30000)
        time.sleep(3)
        title = page.evaluate("""() => {
            let t = document.querySelector('h1') || document.querySelector('.entry-title');
            return t ? t.innerText.trim() : '';
        }""")
        lead = page.evaluate("""() => {
            let ps = Array.from(document.querySelectorAll('.entry-content p, .post-content p, article p'));
            for (const p of ps) {
                let text = p.innerText.trim();
                if (text.length > 30) return text;
            }
            return '';
        }""")

        title = safe_decode(title)
        lead = safe_decode(lead)

        image_url = page.evaluate("""() => {
            let img = document.querySelector('meta[property="og:image"]');
            if (img && img.content) return img.content;
            img = document.querySelector('.wp-post-image, img, article img');
            return img && img.src ? img.src : '';
        }""")
        if not title or not lead:
            log(f"‚ö†Ô∏è avtonovostidnya.ru: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}")
            return None, None, None
        return title.strip(), lead.strip(), image_url
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ avtonovostidnya.ru: {e}")
        return None, None, None

# === –ü–ê–†–°–ò–ù–ì auto.ru ===
def parse_auto_ru(page):
    log("üåê auto.ru/mag/theme/news/")
    try:
        page.goto("https://auto.ru/mag/theme/news/", timeout=60000, wait_until="domcontentloaded")
        for _ in range(7):
            page.mouse.wheel(0, 2000)
            time.sleep(1.2)
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ auto.ru: {e}")
        return []

    try:
        links = page.evaluate("""
            () => Array.from(document.querySelectorAll('a'))
                .map(a => a.href)
                .filter(h => h.includes('/mag/article/') && !h.includes('video') && !h.includes('gallery'))
        """)
        log(f"‚úÖ auto.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return list(set(links))[:5]
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ JS auto.ru: {e}")
        return []

def parse_auto_article(page, url):
    try:
        page.goto(url, timeout=60000, wait_until="domcontentloaded")
        page.wait_for_selector("h1", timeout=10000)
        time.sleep(2)

        title = safe_decode(page.query_selector("h1").inner_text())

        # üö´ –ü—Ä–æ–ø—É—Å–∫, –µ—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –µ—Å—Ç—å "–ì–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å"
        if "–≥–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å" or "–∑–∞–ø—Ä–æ—Å—ã –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –≤—ã, –∞ –Ω–µ —Ä–æ–±–æ—Ç" in title.lower():
            log(f"‚è≠ –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å —Å —Ñ—Ä–∞–∑–æ–π '–ì–ª–∞–≤–Ω–æ–µ –∑–∞ –¥–µ–Ω—å' –∏–ª–∏ '–≤—ã –Ω–µ —Ä–æ–±–æ—Ç': {title}")
            return None, None, None

        image = page.query_selector("meta[property='og:image']")
        image_url = image.get_attribute("content") if image else ""

        lead_text = page.evaluate("""
            () => {
                const raw = document.body.innerText;
                if (!raw) return "";

                const lines = raw
                    .split('\\n')
                    .map(t => t.trim())
                    .filter(t =>
                        t.length > 100 &&
                        !t.toLowerCase().includes("—Ñ–æ—Ç–æ") &&
                        !t.startsWith("–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ") &&
                        !t.toLowerCase().includes("–∏—Å—Ç–æ—á–Ω–∏–∫") &&
                        !t.startsWith("–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ")
                    );

                return lines.length ? lines[0] : "";
            }
        """)

        lead_text = safe_decode(lead_text)

        if not all([title, lead_text, image_url]):
            log(f"‚ö†Ô∏è auto.ru: title={bool(title)}, lead={bool(lead_text)}, img={bool(image_url)}")
            return None, None, None

        return title.strip(), lead_text.strip(), image_url

    except Exception as e:
        log(f"‚ùå auto.ru error: {e}")
        return None, None, None

def llm_check(text1, text2):
    prompt = f"""
–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –æ–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ —ç—Ç–∏ –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ **–æ–¥–Ω–æ –∏ —Ç–æ –∂–µ —Å–æ–±—ã—Ç–∏–µ**, –¥–∞–∂–µ –µ—Å–ª–∏ —Å–ª–æ–≤–∞ –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —Ä–∞–∑–Ω—ã–µ. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ **"–î–∞" –∏–ª–∏ "–ù–µ—Ç"**, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.

–¢–µ–∫—Å—Ç 1:
{text1}

–¢–µ–∫—Å—Ç 2:
{text2}
    """

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    data = {
        "model": "deepseek/deepseek-chat-v3-0324:free",  # –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è DeepSeek —á–µ—Ä–µ–∑ OpenRouter
        "messages": [
            {"role": "user", "content": prompt}
        ]
    }

    try:
        response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, data=json.dumps(data))

        if response.status_code == 200:
            result = response.json()
            answer = result["choices"][0]["message"]["content"].strip().lower()
            log(f"ü§ñ LLM (DeepSeek free) –æ—Ç–≤–µ—Ç: {answer}")
            return "–¥–∞" in answer
        else:
            log(f"‚ùå LLM –æ—à–∏–±–∫–∞: {response.status_code} {response.text}")
            return False

    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM: {e}")
        return False

# === –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ===
def main():
    published = load_published()
    global faiss_index, faiss_texts
    faiss_index, faiss_texts = build_faiss_index(published)
    threading.Thread(target=publish_loop, daemon=True).start()

    while True:
        log("üîÑ –ó–∞–ø—É—Å–∫ –Ω–æ–≤–æ–≥–æ —Ü–∏–∫–ª–∞ —Å–±–æ—Ä–∞ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π...")
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(user_agent="Mozilla/5.0")
            page = context.new_page()

            # –°–±–æ—Ä –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            parsed_articles = []

            # auto.ru
            for link in parse_auto_ru(page):
                result = parse_auto_article(page, link)
                if result:
                    title, lead, image_url = result
                    parsed_articles.append(("auto.ru", title, lead, image_url, link))

            # kolesa.ru
            for link in parse_kolesa_ru(page):
                result = parse_kolesa_article(page, link)
                if result:
                    title, lead, image_url = result
                    parsed_articles.append(("kolesa.ru", title, lead, image_url, link))

            # autostat.ru
            for link in parse_autostat_ru(page):
                result = parse_autostat_article(page, link)
                if result:
                    title, lead, image_url = result
                    parsed_articles.append(("autostat.ru", title, lead, image_url, link))

            # avtonovostidnya.ru
            for link in parse_avtonovostidnya_ru(page):
                result = parse_avtonovostidnya_article(page, link)
                if result:
                    title, lead, image_url = result
                    parsed_articles.append(("avtonovostidnya.ru", title, lead, image_url, link))

            for source, title, lead, image_url, link in parsed_articles:
                if any(isinstance(item, dict) and link == item.get("link") for item in published):
                    log(f"‚è≠ –£–∂–µ –µ—Å—Ç—å {source}: {link}")
                    continue

                log(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ {source}: {link}")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ FAISS
                if is_duplicate(title, lead, published):
                    log(f"üö´ –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç: {link}")
                    continue

                # –ï—Å–ª–∏ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –ø—É–±–ª–∏–∫–∞—Ü–∏–π
                if not title or not lead:
                    log("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫: –ø—É—Å—Ç–æ–π title –∏–ª–∏ lead –≤ main")
                    continue
                text = f"{title.strip()} {lead.strip()}"
                embedding = model.encode(text).tolist()
                enqueue_post(title, lead, image_url, link, source)
                published.append({
                    "link": link,
                    "embedding": embedding,
                    "text": text
                })
                save_published(published)

                # === ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ FAISS ===
                if faiss_index is not None and embedding is not None:
                    import numpy as np
                    emb_array = np.array(embedding, dtype="float32")
                    faiss_index.add(np.expand_dims(emb_array, axis=0))
                    faiss_texts.append((link, f"{title.strip()} {lead.strip()}".lower()))
                    log(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª—ë–Ω: –¥–æ–±–∞–≤–ª–µ–Ω {link}")

            browser.close()
            log("‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –ñ–¥—É 3 —á–∞—Å–∞...\n")

        time.sleep(10800)  # 3 —á–∞—Å–∞

def notify_startup():
    try:
        bot.send_message(
            chat_id=ADMIN_CHAT_ID,
            text="üöÄ –ü–∞—Ä—Å–µ—Ä –∞–≤—Ç–æ–Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–ø—É—â–µ–Ω –∏ –Ω–∞—á–∞–ª —Ä–∞–±–æ—Ç—É.",
            parse_mode="HTML"
        )
        log("üü¢ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–ø—É—Å–∫–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∞–¥–º–∏–Ω—É")
    except Exception as e:
        log(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–ø—É—Å–∫–µ: {e}")

if __name__ == "__main__":
    try:
        notify_startup()
        main()
    except KeyboardInterrupt:
        log("‚õî –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—Ä—É—á–Ω—É—é.")
