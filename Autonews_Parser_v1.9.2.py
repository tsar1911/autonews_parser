import json
import time
import threading
import traceback
import telebot
import random
from playwright.sync_api import sync_playwright
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util
import numpy as np
import logging
import sys
import torch
import requests
import hashlib
from datetime import datetime, timedelta
import telebot.apihelper

log_file = "log_parser.txt"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_file, mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

def log(msg):
    logging.info(msg)

model = SentenceTransformer('all-mpnet-base-v2')
sent_error_hashes = set()
session_embeddings = []
SESSION_EMBEDDING_LIMIT = 50

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
TELEGRAM_TOKEN = ''
TELEGRAM_CHAT_ID = ''
PUBLISHED_FILE = "published_articles.json"
ADMIN_CHAT_ID = ''


bot = telebot.TeleBot(TELEGRAM_TOKEN)
post_queue = []
last_sent_time = 0

def is_valid_image_url(url):
    try:
        r = requests.head(url, timeout=5, allow_redirects=True)
        content_type = r.headers.get("Content-Type", "")
        return content_type.startswith("image/")
    except:
        return False

def notify_admin(text, image_url=None):
    # –•–µ—à–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏
    error_hash = hashlib.md5(text.encode("utf-8")).hexdigest()

    if error_hash in sent_error_hashes:
        log(f"üö´ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–æ—Å—å: {text.splitlines()[0]}")
        return  # –£–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–æ—Å—å

    sent_error_hashes.add(error_hash)

    try:
        if image_url and is_valid_image_url(image_url):
            bot.send_photo(chat_id=ADMIN_CHAT_ID, photo=image_url, caption=text, parse_mode="HTML")
        else:
            bot.send_message(chat_id=ADMIN_CHAT_ID, text=text, parse_mode="HTML")
        log("üîî –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∞–¥–º–∏–Ω—É")
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω—É: {e}")

def load_published():
    try:
        with open(PUBLISHED_FILE, 'r') as f:
            return json.load(f)
    except:
        return []

def save_published(data):
    with open(PUBLISHED_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def is_duplicate(title, lead, published_data, threshold=0.9):
    global session_embeddings

    text = f"{title.strip()} {lead.strip()}"
    embedding = model.encode(text, convert_to_tensor=True, dtype=torch.float32)

    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–º–∏ (–∏–∑ —Ñ–∞–π–ª–∞)
    for item in published_data:
        if "embedding" not in item:
            continue
        stored_embedding = torch.tensor(item["embedding"], dtype=torch.float32)
        similarity = util.cos_sim(embedding, stored_embedding)[0][0].item()
        log(f"üîÅ –°—Ö–æ–¥—Å—Ç–≤–æ —Å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–º: {similarity:.3f} ‚Äî {item['link']}")
        if similarity >= threshold:
            log(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç —Å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–º: {item['link']} ({similarity:.2f})")
            return True

    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —É–∂–µ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –≤ —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
    for emb in session_embeddings:
        similarity = util.cos_sim(embedding, emb)[0][0].item()
        if similarity >= threshold:
            log(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –≤–Ω—É—Ç—Ä–∏ —Å–µ—Å—Å–∏–∏ (–µ—â—ë –Ω–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω) ‚Äî ({similarity:.2f})")
            return True

    # –ï—Å–ª–∏ –≤—Å—ë –æ–∫ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ —Å–µ—Å—Å–∏—é
    session_embeddings.append(embedding)
    if len(session_embeddings) > SESSION_EMBEDDING_LIMIT:
        session_embeddings.pop(0)  # —É–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π
    return False

def enqueue_post(title, lead, image_url, link, source_name):
    if not title or not lead:
        return
    post = {
        "title": title.strip(),
        "lead": lead[:500].strip(),
        "image_url": image_url if image_url and image_url.startswith('http') else "https://via.placeholder.com/800x400?text=No+Image",
        "link": link,
        "source": source_name
    }
    post_queue.append(post)
    log(f"üìù –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {post['title']}")

def publish_loop():
    global last_sent_time
    while True:
        random.shuffle(post_queue)
        if post_queue:
            post = post_queue.pop(0)
            try:
                caption = (
                    f"<b>{post['title']}</b>\n\n"
                    f"{post['lead']}\n\n"
                    f"–ò—Å—Ç–æ—á–Ω–∏–∫: <a href=\"{post['link']}\">{post['source']}</a>"
                )

                if is_valid_image_url(post["image_url"]):
                    bot.send_photo(
                        chat_id=TELEGRAM_CHAT_ID,
                        photo=post["image_url"],
                        caption=caption,
                        parse_mode="HTML"
                    )
                    log(f"üì§ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {post['title']}")
                else:
                    log(f"‚ö†Ô∏è –ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {post['image_url']}. –û—Ç–ø—Ä–∞–≤–∫–∞ –±–µ–∑ —Ñ–æ—Ç–æ.")
                    bot.send_message(
                        chat_id=TELEGRAM_CHAT_ID,
                        text=caption,
                        parse_mode="HTML"
                    )
                    log(f"üì§ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {post['title']}")

                # ‚è±Ô∏è –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ—Ç–ø—Ä–∞–≤–∫–∏
                last_sent_time = time.time()

                # üß† –î–æ–±–∞–≤–ª—è–µ–º –≤ published_articles
                embedding = model.encode(f"{post['title']} {post['lead']}", convert_to_numpy=True).tolist()
                published_data = load_published()
                published_data.append({
                    "title": post["title"],
                    "lead": post["lead"],
                    "link": post["link"],
                    "embedding": embedding,
                    "timestamp": datetime.utcnow().isoformat()
                })
                save_published(published_data)

                # ‚è≥ –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏
                time.sleep(600)

            except telebot.apihelper.ApiTelegramException as e:
                log(f"‚ùå –û—à–∏–±–∫–∞ Telegram API: {e}")
                notify_admin(f"<b>‚ùå –û—à–∏–±–∫–∞ Telegram API</b>\n\n{e}\n\n<i>{post['title']}</i>", post["image_url"])
                post_queue.insert(0, post)
                time.sleep(60)

            except Exception as e:
                log(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
                notify_admin(f"<b>‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</b>\n\n{e}\n\n<i>{post['title']}</i>", post["image_url"])
                post_queue.insert(0, post)
                time.sleep(60)

        else:
            time.sleep(5)

def parse_kolesa_ru(page):
    log("üåê kolesa.ru/news")
    try:
        page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Accept-Language': 'ru-RU,ru;q=0.9'
        })
        page.goto("https://www.kolesa.ru/news", timeout=40000, wait_until="domcontentloaded")
        time.sleep(3)
        links = page.evaluate("""() => {
            const links = new Set();
            const xpathResult = document.evaluate(
                "//a[contains(@href, '/news/') and not(contains(@href, '/news/archive/'))]",
                document,
                null,
                XPathResult.ORDERED_NODE_SNAPSHOT_TYPE,
                null
            );
            for (let i = 0; i < Math.min(xpathResult.snapshotLength, 15); i++) {
                const link = xpathResult.snapshotItem(i);
                if (link.textContent.trim().length > 10) {
                    links.add(link.href.startsWith('http') ? link.href : 'https://www.kolesa.ru' + link.href);
                }
            }
            return Array.from(links);
        }""")
        log(f"‚úÖ kolesa.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return links[:5]
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ kolesa.ru: {e}")
        return []

def parse_kolesa_article(page, url):
    try:
        page.goto(url, timeout=30000, wait_until="domcontentloaded")
        time.sleep(3)

        title = page.locator("h1").first.inner_text(timeout=5000)
        lead = ""
        paragraphs = page.locator("p")
        count = paragraphs.count()
        for i in range(count):
            try:
                text = paragraphs.nth(i).inner_text(timeout=1000).strip()
                if len(text) > 30:
                    lead = text
                    break
            except:
                continue

        image_url = page.locator("meta[property='og:image']").get_attribute("content")
        if not image_url:
            image_url = page.locator("img").first.get_attribute("src")

        if not title or not lead:
            log(f"‚ö†Ô∏è kolesa.ru: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}")
            return None, None, None
        return title.strip(), lead, image_url
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ kolesa.ru: {e}")
        return None, None, None

def parse_autostat_ru(page):
    log("üåê autostat.ru")
    try:
        page.goto("https://www.autostat.ru/news/", timeout=60000)
        time.sleep(5)
        links = page.evaluate("""() => {
            const found = [];
            document.querySelectorAll('a[href^="/news/"]').forEach(a => {
                const href = a.getAttribute('href');
                if (/^\\/news\\/\\d+\\/$/.test(href)) {
                    found.push('https://www.autostat.ru' + href);
                }
            });
            return found.slice(0, 10);
        }""")
        log(f"‚úÖ autostat.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return links[:5]
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ autostat.ru: {str(e)[:200]}")
        return []

def parse_autostat_article(page, url):
    try:
        page.goto(url, timeout=30000)
        time.sleep(3)

        title = page.locator("h1").first.inner_text(timeout=5000)
        lead = ""
        paragraphs = page.locator("p")
        count = paragraphs.count()
        for i in range(count):
            try:
                text = paragraphs.nth(i).inner_text(timeout=1000).strip()
                if (
                    50 < len(text) < 500
                    and "e-mail" not in text.lower()
                    and "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è" not in text.lower()
                    and "–Ω–∞–∂–º–∏—Ç–µ" not in text.lower()
                    and "–ø–æ–¥–ø–∏—Å" not in text.lower()
                    and "–∏—Å—Ç–æ—á–Ω–∏–∫" not in text.lower()
                ):
                    lead = text
                    break
            except:
                continue

        image_url = page.locator("meta[property='og:image']").get_attribute("content")
        if not image_url:
            image_url = page.locator("img").first.get_attribute("src")

        if not title or not lead:
            log(f"‚ö†Ô∏è autostat.ru: –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}")
            return None, None, None
        return title.strip(), lead, image_url
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ autostat.ru: {e}")
        return None, None, None

def parse_avtonovostidnya_ru(page):
    log("üåê avtonovostidnya.ru")
    try:
        page.goto("https://avtonovostidnya.ru/", timeout=30000)
        time.sleep(3)
        links = page.evaluate("""() => {
            const items = Array.from(document.querySelectorAll('article'));
            return items.map(item => {
                const link = item.querySelector('h2 a, h3 a');
                return link ? link.href : null;
            }).filter(href => href && href.includes('avtonovostidnya.ru')).slice(0, 5);
        }""")
        log(f"‚úÖ avtonovostidnya.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return links
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ avtonovostidnya.ru: {str(e)[:200]}")
        return []

def parse_avtonovostidnya_article(page, url):
    try:
        page.goto(url, timeout=30000)
        time.sleep(3)
        title = page.evaluate("""() => {
            let t = document.querySelector('h1') || document.querySelector('.entry-title');
            return t ? t.innerText.trim() : '';
        }""")
        lead = page.evaluate("""() => {
            let ps = Array.from(document.querySelectorAll('.entry-content p, .post-content p, article p'));
            for (const p of ps) {
                let text = p.innerText.trim();
                if (text.length > 30) return text;
            }
            return '';
        }""")
        image_url = page.evaluate("""() => {
            let img = document.querySelector('meta[property="og:image"]');
            if (img && img.content) return img.content;
            img = document.querySelector('.wp-post-image, img, article img');
            return img && img.src ? img.src : '';
        }""")
        if not title or not lead:
            log(f"‚ö†Ô∏è avtonovostidnya.ru: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url}")
            return None, None, None
        return title.strip(), lead.strip(), image_url
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ avtonovostidnya.ru: {e}")
        return None, None, None

# def parse_motor_ru(page):
#     log("üåê motor.ru/pulse")
#     try:
#         page.goto("https://motor.ru/pulse", timeout=60000, wait_until="domcontentloaded")
#         for _ in range(5):
#             page.mouse.wheel(0, 1500)
#             time.sleep(1.5)
#     except Exception as e:
#         log(f"‚ùå –û—à–∏–±–∫–∞ motor.ru: {e}")
#         return []

#     soup = BeautifulSoup(page.content(), "html.parser")
#     links = set()
#     for a in soup.find_all("a", href=True):
#         href = a["href"]
#         if href.startswith("/news") or href.startswith("/pulse"):
#             links.add("https://motor.ru" + href)
#     log(f"‚úÖ motor.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
#     return list(links)[:5]

# def parse_motor_article(page, url):
#     try:
#         page.goto(url, timeout=60000, wait_until="domcontentloaded")
#         page.wait_for_selector("h1", timeout=10000)
#         time.sleep(1)

#         title = page.query_selector("h1").inner_text()
#         lead = page.query_selector("article p")
#         lead_text = lead.inner_text().strip() if lead else ""

#         image = page.query_selector("article img")
#         image_url = image.get_attribute("src") if image else ""

#         if image_url and image_url.startswith("/"):
#             image_url = urljoin(url, image_url)

#         if not title or not lead_text:
#             log(f"‚ö†Ô∏è motor.ru: title={bool(title)}, lead={bool(lead_text)}, img={bool(image_url)}")
#             return title.strip(), lead_text, ""  # –î–∞–∂–µ –µ—Å–ª–∏ –Ω–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç

#         return title.strip(), lead_text, image_url

#     except Exception as e:
#         log(f"‚ùå motor.ru error: {e}")
#         return None, None, None

# === –ü–ê–†–°–ò–ù–ì auto.ru ===
def parse_auto_ru(page):
    log("üåê auto.ru/mag/theme/news/")
    try:
        page.goto("https://auto.ru/mag/theme/news/", timeout=60000, wait_until="domcontentloaded")
        for _ in range(7):
            page.mouse.wheel(0, 2000)
            time.sleep(1.2)
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ auto.ru: {e}")
        return []

    try:
        links = page.evaluate("""
            () => Array.from(document.querySelectorAll('a'))
                .map(a => a.href)
                .filter(h => h.includes('/mag/article/') && !h.includes('video') && !h.includes('gallery'))
        """)
        log(f"‚úÖ auto.ru: {len(links)} —Å—Å—ã–ª–æ–∫")
        return list(set(links))[:5]
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ JS auto.ru: {e}")
        return []

def parse_auto_article(page, url):
    try:
        page.goto(url, timeout=60000, wait_until="domcontentloaded")
        page.wait_for_selector("h1", timeout=10000)
        time.sleep(2)

        title = page.query_selector("h1").inner_text()
        image = page.query_selector("meta[property='og:image']")
        image_url = image.get_attribute("content") if image else ""

        lead_text = page.evaluate("""
            () => {
                const raw = document.body.innerText;
                if (!raw) return "";

                const lines = raw
                    .split('\\n')
                    .map(t => t.trim())
                    .filter(t =>
                        t.length > 100 &&
                        !t.toLowerCase().includes("—Ñ–æ—Ç–æ") &&
                        !t.startsWith("–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ") &&
                        !t.toLowerCase().includes("–∏—Å—Ç–æ—á–Ω–∏–∫") &&
                        !t.startsWith("–°–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ")
                    );

                return lines.length ? lines[0] : "";
            }
        """)

        if not all([title, lead_text, image_url]):
            log(f"‚ö†Ô∏è auto.ru: title={bool(title)}, lead={bool(lead_text)}, img={bool(image_url)}")
            return None, None, None

        return title.strip(), lead_text.strip(), image_url

    except Exception as e:
        log(f"‚ùå auto.ru error: {e}")
        return None, None, None

# === –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ===
def main():
    published = load_published()
    threading.Thread(target=publish_loop, daemon=True).start()

    while True:
        log("üîÑ –ó–∞–ø—É—Å–∫ –Ω–æ–≤–æ–≥–æ —Ü–∏–∫–ª–∞ —Å–±–æ—Ä–∞ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π...")
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(user_agent="Mozilla/5.0")
            page = context.new_page()

            # motor.ru
            # for link in parse_motor_ru(page):
            #     if link in published:
            #         log(f"‚è≠ –£–∂–µ –µ—Å—Ç—å motor.ru: {link}")
            #         continue
            #     log(f"üîç motor.ru: {link}")
            #     title, lead, image_url = parse_motor_article(page, link)
            #     if title and lead and image_url:
            #         enqueue_post(title, lead, image_url, link, "motor.ru")
            #         published.append(link)
            #         save_published(published)

            # auto.ru
            for link in parse_auto_ru(page):
                if any(isinstance(item, dict) and link == item.get("link") for item in published):
                    log(f"‚è≠ –£–∂–µ –µ—Å—Ç—å auto.ru: {link}")
                    continue
                log(f"üîç auto.ru: {link}")
                result = parse_auto_article(page, link)
                if result:
                    title, lead, image_url = result
                    if title and lead and image_url:
                        if not is_duplicate(title, lead, published):
                            embedding = model.encode(f"{title.strip()} {lead.strip()}").tolist()
                            enqueue_post(title, lead, image_url, link, "auto.ru")
                            published.append({"link": link, "embedding": embedding})
                            save_published(published)
                        else:
                            log(f"üö´ –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç: {link}")

            # kolesa.ru
            for link in parse_kolesa_ru(page):
                if any(isinstance(item, dict) and link == item.get("link") for item in published):
                    log(f"‚è≠ –£–∂–µ –µ—Å—Ç—å kolesa.ru: {link}")
                    continue
                log(f"üîç kolesa.ru: {link}")
                result = parse_kolesa_article(page, link)
                if result:
                    title, lead, image_url = result
                    if title and lead and image_url:
                        if not is_duplicate(title, lead, published):
                            embedding = model.encode(f"{title.strip()} {lead.strip()}").tolist()
                            enqueue_post(title, lead, image_url, link, "kolesa.ru")
                            published.append({"link": link, "embedding": embedding})
                            save_published(published)
                        else:
                            log(f"üö´ –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç: {link}")   

            # autostat.ru
            for link in parse_autostat_ru(page):
                if any(isinstance(item, dict) and link == item.get("link") for item in published):
                    log(f"‚è≠ –£–∂–µ –µ—Å—Ç—å autostat.ru: {link}")
                    continue
                log(f"üîç autostat.ru: {link}")
                result = parse_autostat_article(page, link)
                if result:
                    title, lead, image_url = result
                    if title and lead and image_url:
                        if not is_duplicate(title, lead, published):
                            embedding = model.encode(f"{title.strip()} {lead.strip()}").tolist()
                            enqueue_post(title, lead, image_url, link, "autostat.ru")
                            published.append({"link": link, "embedding": embedding})
                            save_published(published)
                        else:
                            log(f"üö´ –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç: {link}")

            # avtonovostidnya.ru
            for link in parse_avtonovostidnya_ru(page):
                if any(isinstance(item, dict) and link == item.get("link") for item in published):
                    log(f"‚è≠ –£–∂–µ –µ—Å—Ç—å avtonovostidnya.ru: {link}")
                    continue
                log(f"üîç avtonovostidnya.ru: {link}")
                result = parse_avtonovostidnya_article(page, link)
                if result:
                    title, lead, image_url = result
                    if title and lead and image_url:
                        if not is_duplicate(title, lead, published):
                            embedding = model.encode(f"{title.strip()} {lead.strip()}").tolist()
                            enqueue_post(title, lead, image_url, link, "avtonovostidnya.ru")
                            published.append({"link": link, "embedding": embedding})
                            save_published(published)
                        else:
                            log(f"üö´ –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç: {link}")
                            
            browser.close()
            log("‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –ñ–¥—É 1 —á–∞—Å...\n")

        time.sleep(3600)  # 30 –º–∏–Ω—É—Ç

def notify_startup():
    try:
        bot.send_message(
            chat_id=ADMIN_CHAT_ID,
            text="üöÄ –ü–∞—Ä—Å–µ—Ä –∞–≤—Ç–æ–Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–ø—É—â–µ–Ω –∏ –Ω–∞—á–∞–ª —Ä–∞–±–æ—Ç—É.",
            parse_mode="HTML"
        )
        log("üü¢ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–ø—É—Å–∫–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∞–¥–º–∏–Ω—É")
    except Exception as e:
        log(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–ø—É—Å–∫–µ: {e}")

if __name__ == "__main__":
    try:
        notify_startup()
        main()
    except KeyboardInterrupt:
        log("‚õî –°–∫—Ä–∏–ø—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—Ä—É—á–Ω—É—é.")
